{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.6043052550623464\n",
      "Testing score: 0.5339174600378664\n",
      "Using 124 features, and 1 (price)target\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Reading our Immoweb dataset in 'df'.\n",
    "df = pd.read_csv('../data/merged_data.csv')\n",
    "\n",
    "# Instantiating LinearRegression as 'reg'.\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Filling NaN values with 0.\n",
    "df['landplot'].fillna(0, inplace=True)\n",
    "df['facades'].fillna(0, inplace=True)\n",
    "df['Living area'].fillna(0, inplace=True)\n",
    "\n",
    "# Reduces zip codes to 2 digits for broader scope.\n",
    "df['Zip code'] = (df['Zip code']/100).astype(int)\n",
    "\n",
    "# Creating dummy columns from categorical data.\n",
    "df = pd.get_dummies(df, columns=['condition', 'province', 'Zip code', 'subtype'])\n",
    "\n",
    "# Removing features that we won't be using.\n",
    "df.drop(['city', 'Kitchen', 'Terrace', 'type'], axis=1, inplace=True)\n",
    "\n",
    "# Because 'get_dummies()' creates boolean values, we re-define our dataframe to be integers only.\n",
    "df = df.astype(int)\n",
    "\n",
    "# Shows the 10 first rows of the cleaned dataframe.\n",
    "#display(df.head(10))\n",
    "\n",
    "# Defining 'X' and 'y' variables from our dataframe using purely features that contain numerical data.\n",
    "X = df.drop(['price'], axis=1).to_numpy()\n",
    "y = df['price'].to_numpy()\n",
    "\n",
    "# Reshaping 'y' to be 2D array.\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Setting up 'train_test_split' to get standardized training/testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Training our model.\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Displaying score of Training variables.\n",
    "print(\"Training score:\", reg.score(X_train, y_train)) \n",
    "\n",
    "# Predicting the 'y' target value (Price).\n",
    "y_prediction = reg.predict(X_test)\n",
    "\n",
    "# Displaying the score of Testing variables\n",
    "features = X.shape[1]\n",
    "print(\"Testing score:\", reg.score(X_test, y_test))\n",
    "print(f\"Using {features} features, and 1 (price)target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR Training score: 0.8106913069516318\n",
      "DTR Testing score: 0.6102132000228297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Instatiate 'DecisionTreeRegressor()' and setting parameters.\n",
    "dtr = DecisionTreeRegressor(max_depth=50, max_leaf_nodes=66, ccp_alpha=0.1)\n",
    "\n",
    "dtr.fit(X_train, y_train)\n",
    "print('DTR Training score:', dtr.score(X_train, y_train))\n",
    "y_pred = dtr.predict(X_test)\n",
    "print('DTR Testing score:', dtr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[15:33:13] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\objective\\multiclass_obj.cu:123: SoftmaxMultiClassObj: label must be in [0, num_class).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[535], line 16\u001b[0m\n\u001b[0;32m      9\u001b[0m param \u001b[39m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m4\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[39m'\u001b[39m\u001b[39meta\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m0.3\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mmulti:softmax\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_class\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y))}\n\u001b[0;32m     14\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m---> 16\u001b[0m model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39;49mtrain(param, train, num_boost_round\u001b[39m=\u001b[39;49mepochs)\n\u001b[0;32m     17\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test)\n\u001b[0;32m     18\u001b[0m accuracy_score(y_test, predictions)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\Immo-Project-Data-Analysis\\.analysis_environment\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\Immo-Project-Data-Analysis\\.analysis_environment\\Lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\Immo-Project-Data-Analysis\\.analysis_environment\\Lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\Immo-Project-Data-Analysis\\.analysis_environment\\Lib\\site-packages\\xgboost\\core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [15:33:13] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\objective\\multiclass_obj.cu:123: SoftmaxMultiClassObj: label must be in [0, num_class)."
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Attempt at using XGBoost\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'max_depth':4,\n",
    "    'eta':0.3,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class':len(np.unique(y))}\n",
    "epochs = 10\n",
    "\n",
    "model = xgb.train(param, train, num_boost_round=epochs)\n",
    "predictions = model.predict(test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".analysis_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
